# Simple LlamaCPP Chatbot ğŸ¥

Un chatbot simple et personnalisable utilisant llama.cpp avec le modÃ¨le CroissantLLM, facilement embeddable sur n'importe quel site web.

## ğŸš€ FonctionnalitÃ©s

- Interface de chat simple et responsive
- Utilise le modÃ¨le CroissantLLM franÃ§ais
- Personnalisable (couleurs, titre, placeholder, etc.)
- Embeddable sur n'importe quel site
- Backend Express.js simple
- Support pour llama.cpp

## ğŸ“‹ PrÃ©requis

- Node.js (v14 ou plus rÃ©cent)
- llama.cpp installÃ©
- Le modÃ¨le CroissantLLM tÃ©lÃ©chargÃ©

## ğŸ› ï¸ Installation

1. **Cloner le repository**
   ```bash
   git clone https://github.com/creach-t/simple-llamacpp-chatbot.git
   cd simple-llamacpp-chatbot
   ```

2. **Installer les dÃ©pendances**
   ```bash
   npm install
   ```

3. **Installer llama.cpp** (si ce n'est pas dÃ©jÃ  fait)
   ```bash
   git clone https://github.com/ggerganov/llama.cpp.git
   cd llama.cpp
   make
   ```

4. **TÃ©lÃ©charger le modÃ¨le CroissantLLM**
   ```bash
   # CrÃ©er le dossier models
   mkdir -p models
   
   # TÃ©lÃ©charger le modÃ¨le (exemple avec wget)
   wget -O models/croissant.gguf https://huggingface.co/croissantllm/CroissantLLMChat-v0.1-GGUF/resolve/main/croissant-llm-chat-v0.1.Q4_K_M.gguf
   ```

5. **Configurer les chemins**
   
   Modifier le fichier `config.json` selon votre installation :
   ```json
   {
     "llamaCppPath": "/path/to/llama.cpp/llama-cli",
     "modelPath": "./models/croissant.gguf"
   }
   ```

## ğŸš€ Utilisation

### DÃ©marrer le serveur

```bash
npm start
```

Le serveur sera accessible sur `http://localhost:3000`

### Embed sur votre site

#### Option 1 : Iframe
```html
<iframe 
  src="http://localhost:3000/embed" 
  width="400" 
  height="600" 
  frameborder="0">
</iframe>
```

#### Option 2 : Widget JavaScript
```html
<div id="chatbot-container"></div>
<script src="http://localhost:3000/widget.js"></script>
<script>
  ChatbotWidget.init({
    container: '#chatbot-container',
    title: 'Mon Chatbot',
    primaryColor: '#007bff',
    placeholder: 'Tapez votre message...'
  });
</script>
```

## âš™ï¸ Configuration

Le fichier `config.json` permet de personnaliser :

- `port` : Port du serveur (dÃ©faut: 3000)
- `llamaCppPath` : Chemin vers l'exÃ©cutable llama-cli
- `modelPath` : Chemin vers le modÃ¨le .gguf
- `maxTokens` : Nombre maximum de tokens gÃ©nÃ©rÃ©s (dÃ©faut: 100)
- `temperature` : CrÃ©ativitÃ© du modÃ¨le (0.0 Ã  1.0)

## ğŸ“ Structure du projet

```
simple-llamacpp-chatbot/
â”œâ”€â”€ server.js           # Serveur Express principal
â”œâ”€â”€ config.json         # Configuration
â”œâ”€â”€ package.json        # DÃ©pendances npm
â”œâ”€â”€ public/            
â”‚   â”œâ”€â”€ index.html      # Interface principale
â”‚   â”œâ”€â”€ embed.html      # Version embeddable
â”‚   â”œâ”€â”€ widget.js       # Widget JavaScript
â”‚   â””â”€â”€ style.css       # Styles CSS
â””â”€â”€ models/             # Dossier pour les modÃ¨les
```

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou soumettre une pull request.

## ğŸ“„ Licence

MIT License
