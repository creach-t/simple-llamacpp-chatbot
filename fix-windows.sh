#!/bin/bash

# Script de correction pour Windows avec URLs sp√©cifiques
echo "üîß Correction pour Windows..."

# V√©rifier si on est sur Windows
if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "cygwin" ]]; then
    echo "‚úÖ D√©tection de Windows"
    
    # Cr√©er le dossier llama.cpp s'il n'existe pas
    mkdir -p llama.cpp
    
    echo "üì• T√©l√©chargement des binaires llama.cpp pour Windows..."
    
    # URL directe des binaires Windows
    DOWNLOAD_URL="https://github.com/ggml-org/llama.cpp/releases/download/b5604/llama-b5604-bin-win-cpu-x64.zip"
    
    echo "üì¶ T√©l√©chargement depuis: $DOWNLOAD_URL"
    
    # T√©l√©charger
    curl -L -o llama-cpp-windows.zip "$DOWNLOAD_URL"
    
    # Extraire dans le dossier llama.cpp
    if command -v unzip &> /dev/null; then
        unzip -o llama-cpp-windows.zip -d llama.cpp
    elif command -v 7z &> /dev/null; then
        7z x llama-cpp-windows.zip -ollama.cpp -y
    else
        echo "‚ö†Ô∏è  Extraction manuelle requise"
        echo "Veuillez extraire llama-cpp-windows.zip dans le dossier llama.cpp/"
        exit 1
    fi
    
    # Nettoyer
    rm -f llama-cpp-windows.zip
    
    # Chercher l'ex√©cutable (llama-cli.exe ou main.exe)
    LLAMACLI_PATH=$(find llama.cpp -name "llama-cli.exe" -o -name "main.exe" -o -name "llama.exe" | head -n 1)
    
    if [ -n "$LLAMACLI_PATH" ]; then
        echo "‚úÖ Ex√©cutable trouv√©: $LLAMACLI_PATH"
        
        # Mettre √† jour la configuration avec le chemin Windows
        WINDOWS_PATH=$(echo "$LLAMACLI_PATH" | sed 's|/|\\|g')
        
        # Cr√©er un nouveau config.json avec le bon chemin
        cat > config.json << EOF
{
  "port": 3000,
  "llamaCppPath": "./$LLAMACLI_PATH",
  "modelPath": "./models/croissant.gguf",
  "maxTokens": 100,
  "temperature": 0.7,
  "chatbot": {
    "title": "Assistant CroissantLLM",
    "placeholder": "Tapez votre message en fran√ßais...",
    "primaryColor": "#4A90E2",
    "secondaryColor": "#F5F5F5",
    "textColor": "#333333",
    "maxMessages": 50,
    "typingDelay": 100
  },
  "llamaArgs": {
    "ctx_size": 2048,
    "threads": 4,
    "batch_size": 512
  }
}
EOF
        
        echo "‚úÖ Configuration mise √† jour avec le chemin: ./$LLAMACLI_PATH"
        
    else
        echo "‚ùå Aucun ex√©cutable trouv√© dans l'archive"
        echo "üìÇ Contenu du dossier llama.cpp:"
        ls -la llama.cpp/
        exit 1
    fi
    
else
    echo "‚ùå Ce script est destin√© √† Windows"
    exit 1
fi

# T√©l√©charger le mod√®le s'il n'existe pas
if [ ! -f "models/croissant.gguf" ]; then
    echo "üì• T√©l√©chargement du mod√®le CroissantLLM..."
    mkdir -p models
    
    # URL directe du mod√®le CroissantLLM
    MODEL_URL="https://huggingface.co/croissantllm/CroissantLLMChat-v0.1-GGUF/resolve/main/croissantllmchat-v0.1.Q4_K_M.gguf?download=true"
    
    echo "üì¶ T√©l√©chargement du mod√®le depuis HuggingFace..."
    echo "‚è≥ Cela peut prendre plusieurs minutes (fichier ~2.4GB)..."
    
    if command -v wget &> /dev/null; then
        wget -O "models/croissant.gguf" "$MODEL_URL" --progress=bar:force
    elif command -v curl &> /dev/null; then
        curl -L -o "models/croissant.gguf" "$MODEL_URL" --progress-bar
    else
        echo "‚ö†Ô∏è  wget ou curl requis pour t√©l√©charger le mod√®le"
        echo "Veuillez t√©l√©charger manuellement le mod√®le depuis:"
        echo "$MODEL_URL"
        echo "Et le placer dans: models/croissant.gguf"
    fi
    
    # V√©rifier la taille du fichier t√©l√©charg√©
    if [ -f "models/croissant.gguf" ]; then
        SIZE=$(du -h "models/croissant.gguf" | cut -f1)
        echo "‚úÖ Mod√®le t√©l√©charg√© avec succ√®s (taille: $SIZE)"
    fi
else
    echo "‚úÖ Mod√®le d√©j√† pr√©sent"
fi

# Test de l'installation
echo "üß™ Test de la configuration..."
if [ -f "$LLAMACLI_PATH" ] && [ -f "models/croissant.gguf" ]; then
    echo "‚úÖ Tous les fichiers sont pr√©sents"
    
    # Test rapide de llama.cpp
    echo "üîç Test de llama.cpp..."
    timeout 5s "./$LLAMACLI_PATH" --help > /dev/null 2>&1
    if [ $? -eq 0 ] || [ $? -eq 124 ]; then  # 124 = timeout (normal)
        echo "‚úÖ llama.cpp fonctionne correctement"
    else
        echo "‚ö†Ô∏è  Test de llama.cpp partiellement r√©ussi"
    fi
else
    echo "‚ö†Ô∏è  Certains fichiers manquent, v√©rifiez l'installation"
fi

echo
echo "üéâ Configuration termin√©e pour Windows !"
echo "========================================="
echo
echo "Pour d√©marrer le chatbot :"
echo "  npm start"
echo
echo "URLs disponibles :"
echo "  ‚Ä¢ Interface principale:  http://localhost:3000"
echo "  ‚Ä¢ Version embeddable:    http://localhost:3000/embed"
echo "  ‚Ä¢ Exemples:              http://localhost:3000/examples/example.html"
echo
echo "Configuration :"
echo "  ‚Ä¢ Ex√©cutable: ./$LLAMACLI_PATH"
echo "  ‚Ä¢ Mod√®le: ./models/croissant.gguf"
echo
