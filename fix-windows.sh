#!/bin/bash

# Script de correction pour Windows
echo "üîß Correction pour Windows..."

# V√©rifier si on est sur Windows
if [[ "$OSTYPE" == "msys" || "$OSTYPE" == "cygwin" ]]; then
    echo "‚úÖ D√©tection de Windows"
    
    # Cr√©er le dossier llama.cpp s'il n'existe pas
    mkdir -p llama.cpp
    
    echo "üì• T√©l√©chargement des binaires llama.cpp pour Windows..."
    
    # URL des releases GitHub
    RELEASE_URL="https://api.github.com/repos/ggerganov/llama.cpp/releases/latest"
    
    # Obtenir l'URL de t√©l√©chargement du binaire Windows
    DOWNLOAD_URL=$(curl -s "$RELEASE_URL" | grep "browser_download_url.*win.*x64.*zip" | cut -d '"' -f 4 | head -n 1)
    
    if [ -n "$DOWNLOAD_URL" ]; then
        echo "üì¶ T√©l√©chargement depuis: $DOWNLOAD_URL"
        
        # T√©l√©charger
        curl -L -o llama-cpp-windows.zip "$DOWNLOAD_URL"
        
        # Extraire dans le dossier llama.cpp
        if command -v unzip &> /dev/null; then
            unzip -o llama-cpp-windows.zip -d llama.cpp
        elif command -v 7z &> /dev/null; then
            7z x llama-cpp-windows.zip -ollama.cpp -y
        else
            echo "‚ö†Ô∏è  Extraction manuelle requise"
            echo "Veuillez extraire llama-cpp-windows.zip dans le dossier llama.cpp/"
            exit 1
        fi
        
        # Nettoyer
        rm -f llama-cpp-windows.zip
        
        # Chercher l'ex√©cutable
        LLAMACLI_PATH=$(find llama.cpp -name "llama-cli.exe" -o -name "main.exe" -o -name "llama.exe" | head -n 1)
        
        if [ -n "$LLAMACLI_PATH" ]; then
            echo "‚úÖ Ex√©cutable trouv√©: $LLAMACLI_PATH"
            
            # Mettre √† jour la configuration
            FULL_PATH=$(realpath "$LLAMACLI_PATH")
            
            # Cr√©er un nouveau config.json avec le bon chemin
            cat > config.json << EOF
{
  "port": 3000,
  "llamaCppPath": "$FULL_PATH",
  "modelPath": "./models/croissant.gguf",
  "maxTokens": 100,
  "temperature": 0.7,
  "chatbot": {
    "title": "Assistant CroissantLLM",
    "placeholder": "Tapez votre message en fran√ßais...",
    "primaryColor": "#4A90E2",
    "secondaryColor": "#F5F5F5",
    "textColor": "#333333",
    "maxMessages": 50,
    "typingDelay": 100
  },
  "llamaArgs": {
    "ctx_size": 2048,
    "threads": 4,
    "batch_size": 512
  }
}
EOF
            
            echo "‚úÖ Configuration mise √† jour"
            echo "üöÄ Vous pouvez maintenant d√©marrer le serveur avec: npm start"
            
        else
            echo "‚ùå Aucun ex√©cutable trouv√© dans l'archive"
            exit 1
        fi
        
    else
        echo "‚ùå Impossible de trouver l'URL de t√©l√©chargement"
        exit 1
    fi
    
else
    echo "‚ùå Ce script est destin√© √† Windows"
    exit 1
fi

# T√©l√©charger le mod√®le s'il n'existe pas
if [ ! -f "models/croissant.gguf" ]; then
    echo "üì• T√©l√©chargement du mod√®le CroissantLLM..."
    mkdir -p models
    
    MODEL_URL="https://huggingface.co/croissantllm/CroissantLLMChat-v0.1-GGUF/resolve/main/croissant-llm-chat-v0.1.Q4_K_M.gguf"
    
    if command -v wget &> /dev/null; then
        wget -O "models/croissant.gguf" "$MODEL_URL"
    elif command -v curl &> /dev/null; then
        curl -L -o "models/croissant.gguf" "$MODEL_URL"
    else
        echo "‚ö†Ô∏è  Veuillez t√©l√©charger manuellement le mod√®le depuis:"
        echo "$MODEL_URL"
        echo "Et le placer dans: models/croissant.gguf"
    fi
fi

echo "üéâ Configuration termin√©e !"
