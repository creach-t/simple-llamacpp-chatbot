#!/bin/bash

# Script d'installation automatique pour Simple LlamaCPP Chatbot
# Usage: bash install.sh

set -e

echo "ü•ê Installation du Chatbot CroissantLLM"
echo "======================================="

# Couleurs pour l'affichage
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Fonction pour afficher les messages
info() {
    echo -e "${BLUE}‚ÑπÔ∏è  $1${NC}"
}

success() {
    echo -e "${GREEN}‚úÖ $1${NC}"
}

warning() {
    echo -e "${YELLOW}‚ö†Ô∏è  $1${NC}"
}

error() {
    echo -e "${RED}‚ùå $1${NC}"
    exit 1
}

# V√©rifier si Node.js est install√©
check_node() {
    info "V√©rification de Node.js..."
    if ! command -v node &> /dev/null; then
        error "Node.js n'est pas install√©. Veuillez l'installer depuis https://nodejs.org"
    fi
    
    NODE_VERSION=$(node --version | cut -c2-)
    REQUIRED_VERSION="14.0.0"
    
    if [ "$(printf '%s\n' "$REQUIRED_VERSION" "$NODE_VERSION" | sort -V | head -n1)" = "$REQUIRED_VERSION" ]; then
        success "Node.js version $NODE_VERSION d√©tect√©e"
    else
        error "Node.js version $REQUIRED_VERSION ou sup√©rieure requise, version $NODE_VERSION d√©tect√©e"
    fi
}

# Installer les d√©pendances npm
install_dependencies() {
    info "Installation des d√©pendances npm..."
    if [ -f "package.json" ]; then
        npm install
        success "D√©pendances install√©es"
    else
        error "Fichier package.json non trouv√©"
    fi
}

# V√©rifier/installer llama.cpp
setup_llamacpp() {
    info "Configuration de llama.cpp..."
    
    if [ ! -d "llama.cpp" ]; then
        warning "llama.cpp non trouv√©, t√©l√©chargement en cours..."
        git clone https://github.com/ggerganov/llama.cpp.git
        cd llama.cpp
        
        info "Compilation de llama.cpp..."
        make -j4
        cd ..
        success "llama.cpp install√© et compil√©"
    else
        success "llama.cpp d√©j√† pr√©sent"
    fi
    
    # V√©rifier l'ex√©cutable
    if [ -f "llama.cpp/llama-cli" ]; then
        success "Ex√©cutable llama-cli trouv√©"
    elif [ -f "llama.cpp/main" ]; then
        warning "Ancien nom d'ex√©cutable d√©tect√©, cr√©ation d'un lien symbolique..."
        ln -sf main llama.cpp/llama-cli
        success "Lien symbolique cr√©√©"
    else
        error "Ex√©cutable llama.cpp non trouv√© apr√®s compilation"
    fi
}

# T√©l√©charger le mod√®le CroissantLLM
download_model() {
    info "Configuration du mod√®le CroissantLLM..."
    
    # Cr√©er le dossier models
    mkdir -p models
    
    MODEL_FILE="models/croissant.gguf"
    
    if [ ! -f "$MODEL_FILE" ]; then
        warning "Mod√®le CroissantLLM non trouv√©, t√©l√©chargement en cours..."
        
        # URL du mod√®le (quantifi√© Q4_K_M pour un bon √©quilibre taille/qualit√©)
        MODEL_URL="https://huggingface.co/croissantllm/CroissantLLMChat-v0.1-GGUF/resolve/main/croissant-llm-chat-v0.1.Q4_K_M.gguf"
        
        info "T√©l√©chargement depuis HuggingFace..."
        info "Cela peut prendre plusieurs minutes selon votre connexion..."
        
        if command -v wget &> /dev/null; then
            wget -O "$MODEL_FILE" "$MODEL_URL" --progress=bar:force
        elif command -v curl &> /dev/null; then
            curl -L -o "$MODEL_FILE" "$MODEL_URL" --progress-bar
        else
            error "wget ou curl requis pour t√©l√©charger le mod√®le"
        fi
        
        success "Mod√®le t√©l√©charg√©: $MODEL_FILE"
    else
        success "Mod√®le d√©j√† pr√©sent: $MODEL_FILE"
    fi
    
    # V√©rifier la taille du fichier
    if [ -f "$MODEL_FILE" ]; then
        SIZE=$(du -h "$MODEL_FILE" | cut -f1)
        info "Taille du mod√®le: $SIZE"
    fi
}

# Configurer les chemins
configure_paths() {
    info "Configuration des chemins..."
    
    # Chemin absolu vers llama.cpp
    LLAMA_PATH=$(realpath llama.cpp/llama-cli)
    MODEL_PATH=$(realpath models/croissant.gguf)
    
    # Cr√©er ou mettre √† jour config.json
    cat > config.json << EOF
{
  "port": 3000,
  "llamaCppPath": "$LLAMA_PATH",
  "modelPath": "$MODEL_PATH",
  "maxTokens": 100,
  "temperature": 0.7,
  "chatbot": {
    "title": "Assistant CroissantLLM",
    "placeholder": "Tapez votre message en fran√ßais...",
    "primaryColor": "#4A90E2",
    "secondaryColor": "#F5F5F5",
    "textColor": "#333333",
    "maxMessages": 50,
    "typingDelay": 100
  },
  "llamaArgs": {
    "ctx_size": 2048,
    "threads": 4,
    "batch_size": 512
  }
}
EOF
    
    success "Configuration mise √† jour"
    info "Chemin llama.cpp: $LLAMA_PATH"
    info "Chemin mod√®le: $MODEL_PATH"
}

# Test de fonctionnement
test_setup() {
    info "Test de la configuration..."
    
    # Test basique de llama.cpp
    if [ -f "llama.cpp/llama-cli" ] && [ -f "models/croissant.gguf" ]; then
        info "Test de llama.cpp avec le mod√®le..."
        
        timeout 10s ./llama.cpp/llama-cli -m models/croissant.gguf -p "Bonjour" -n 5 --no-display-prompt > /dev/null 2>&1
        
        if [ $? -eq 0 ] || [ $? -eq 124 ]; then  # 124 = timeout (normal)
            success "Test r√©ussi - llama.cpp fonctionne avec le mod√®le"
        else
            warning "Test partiellement r√©ussi - v√©rifiez manuellement"
        fi
    else
        warning "Impossible de tester - fichiers manquants"
    fi
}

# Cr√©er un script de d√©marrage
create_start_script() {
    info "Cr√©ation du script de d√©marrage..."
    
    cat > start.sh << 'EOF'
#!/bin/bash

echo "ü•ê D√©marrage du Chatbot CroissantLLM..."

# V√©rifier que tout est en place
if [ ! -f "config.json" ]; then
    echo "‚ùå Fichier config.json manquant"
    exit 1
fi

if [ ! -f "server.js" ]; then
    echo "‚ùå Fichier server.js manquant"
    exit 1
fi

# D√©marrer le serveur
echo "üöÄ Lancement du serveur..."
node server.js
EOF
    
    chmod +x start.sh
    success "Script de d√©marrage cr√©√©: ./start.sh"
}

# Afficher les instructions finales
show_instructions() {
    echo
    echo "üéâ Installation termin√©e !"
    echo "========================"
    echo
    echo "Pour d√©marrer le chatbot :"
    echo "  npm start"
    echo "  # ou"
    echo "  ./start.sh"
    echo
    echo "URLs disponibles :"
    echo "  ‚Ä¢ Interface principale:  http://localhost:3000"
    echo "  ‚Ä¢ Version embeddable:    http://localhost:3000/embed"
    echo "  ‚Ä¢ Exemple d'int√©gration: http://localhost:3000/examples/example.html"
    echo "  ‚Ä¢ Widget JavaScript:     http://localhost:3000/widget.js"
    echo
    echo "Configuration :"
    echo "  ‚Ä¢ Fichier: config.json"
    echo "  ‚Ä¢ Mod√®le: $(basename $(grep -o '"modelPath": "[^"]*"' config.json | cut -d'"' -f4) 2>/dev/null || echo 'models/croissant.gguf')"
    echo "  ‚Ä¢ Threads: $(grep -o '"threads": [0-9]*' config.json | cut -d':' -f2 | tr -d ' ' 2>/dev/null || echo '4')"
    echo
    echo "Pour personnaliser, √©ditez le fichier config.json"
    echo
    warning "Premi√®re utilisation: le mod√®le peut prendre quelques secondes √† se charger"
    echo
}

# Fonction principale
main() {
    echo
    info "D√©but de l'installation..."
    echo
    
    check_node
    install_dependencies
    setup_llamacpp
    download_model
    configure_paths
    test_setup
    create_start_script
    
    echo
    show_instructions
}

# V√©rifier si on est dans le bon dossier
if [ ! -f "package.json" ] || [ ! -f "server.js" ]; then
    error "Veuillez ex√©cuter ce script depuis le dossier racine du projet"
fi

# Demander confirmation avant installation
echo
read -p "Voulez-vous continuer l'installation ? (y/N) " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    main
else
    info "Installation annul√©e"
    exit 0
fi
