const express = require('express');
const cors = require('cors');
const bodyParser = require('body-parser');
const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs');

// Charger la configuration
const config = JSON.parse(fs.readFileSync('./config.json', 'utf8'));

const app = express();

// Middleware
app.use(cors());
app.use(bodyParser.json());
app.use(express.static('public'));

// Route principale
app.get('/', (req, res) => {
    res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

// Route pour la version embeddable
app.get('/embed', (req, res) => {
    res.sendFile(path.join(__dirname, 'public', 'embed.html'));
});

// Route pour r√©cup√©rer la configuration c√¥t√© client
app.get('/api/config', (req, res) => {
    res.json({
        chatbot: config.chatbot
    });
});

// Fonction pour formater le prompt selon le template
function formatPrompt(message, history, templateType, systemPrompt = null) {
    let prompt = '';
    
    switch (templateType) {
        case 'vigogne_chat':
            // Template Vigogne Chat: <|UTILISATEUR|>: ... <|ASSISTANT|>:
            
            // Ajouter le contexte syst√®me si fourni
            if (systemPrompt) {
                prompt += `<|ASSISTANT|>: ${systemPrompt}\n\n`;
            }
            
            // Ajouter l'historique
            if (history.length > 0) {
                history.forEach(msg => {
                    if (msg.type === 'user') {
                        prompt += `<|UTILISATEUR|>: ${msg.content}\n`;
                    } else if (msg.type === 'bot') {
                        prompt += `<|ASSISTANT|>: ${msg.content}\n`;
                    }
                });
            }
            prompt += `<|UTILISATEUR|>: ${message}\n<|ASSISTANT|>:`;
            break;
            
        case 'vigogne_instruct':
            // Template Vigogne Instruct: ### Instruction: ... ### Response:
            let instruction = message;
            if (systemPrompt) {
                instruction = `${systemPrompt}\n\n${message}`;
            }
            prompt = `Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n${instruction}\n\n### Response:`;
            break;
            
        case 'chatml':
        default:
            // Template ChatML (Qwen, CroissantLLM): <|im_start|>user ... <|im_end|>
            
            // Ajouter le contexte syst√®me si fourni
            if (systemPrompt) {
                prompt += `<|im_start|>system\n${systemPrompt}<|im_end|>\n`;
            }
            
            // Ajouter l'historique
            if (history.length > 0) {
                history.forEach(msg => {
                    if (msg.type === 'user') {
                        prompt += `<|im_start|>user\n${msg.content}<|im_end|>\n`;
                    } else if (msg.type === 'bot') {
                        prompt += `<|im_start|>assistant\n${msg.content}<|im_end|>\n`;
                    }
                });
            }
            prompt += `<|im_start|>user\n${message}<|im_end|>\n<|im_start|>assistant\n`;
            break;
    }
    
    return prompt;
}

// Fonction pour nettoyer la sortie selon le template
function cleanOutput(output, templateType) {
    let cleanOutput = output;
    
    // Nettoyage g√©n√©ral pour tous les templates
    cleanOutput = cleanOutput
        .replace(/\[end of text\]/gi, '')           // Supprimer [end of text]
        .replace(/<\|endoftext\|>/gi, '')          // Supprimer <|endoftext|>
        .replace(/<\/s>/gi, '')                     // Supprimer </s>
        .replace(/<s>/gi, '')                       // Supprimer <s>
        .replace(/\[EOS\]/gi, '')                   // Supprimer [EOS]
        .replace(/\[\/INST\]/gi, '')                // Supprimer [/INST]
        .replace(/\<\|eot_id\|\>/gi, '')            // Supprimer <|eot_id|>
        .replace(/\n\s*\n\s*\n/g, '\n\n')          // R√©duire multiples sauts de ligne
        .trim();
    
    // Nettoyage sp√©cifique selon le template
    switch (templateType) {
        case 'vigogne_chat':
            cleanOutput = cleanOutput
                .replace(/<\|UTILISATEUR\|>:.*$/g, '')   // Supprimer r√©p√©titions utilisateur
                .replace(/<\|ASSISTANT\|>:/g, '')        // Supprimer le pr√©fixe assistant
                .replace(/^[\s\n]+/g, '')                // Supprimer espaces en d√©but
                .trim();
            break;
            
        case 'vigogne_instruct':
            cleanOutput = cleanOutput
                .replace(/### Instruction:.*$/g, '')     // Supprimer r√©p√©titions instruction
                .replace(/### Response:/g, '')           // Supprimer le pr√©fixe response
                .trim();
            break;
            
        case 'chatml':
        default:
            cleanOutput = cleanOutput
                .replace(/<\|im_end\|>/g, '')            // Supprimer im_end
                .replace(/<\|im_start\|>.*$/g, '')       // Supprimer r√©p√©titions im_start
                .trim();
            break;
    }
    
    // Supprimer les lignes vides au d√©but et √† la fin
    cleanOutput = cleanOutput.replace(/^\s+|\s+$/g, '');
    
    return cleanOutput;
}

// Route pour d√©finir le contexte syst√®me
app.post('/api/context', (req, res) => {
    try {
        const { context } = req.body;
        
        // Sauvegarder le contexte dans un fichier temporaire
        const contextData = {
            context: context || '',
            timestamp: new Date().toISOString()
        };
        
        fs.writeFileSync('./context.json', JSON.stringify(contextData, null, 2));
        
        res.json({ 
            status: 'success',
            message: 'Contexte mis √† jour',
            context: context
        });
        
    } catch (error) {
        console.error('Erreur lors de la mise √† jour du contexte:', error);
        res.status(500).json({ 
            error: 'Erreur lors de la mise √† jour du contexte',
            details: error.message 
        });
    }
});

// Route pour r√©cup√©rer le contexte actuel
app.get('/api/context', (req, res) => {
    try {
        let context = '';
        
        if (fs.existsSync('./context.json')) {
            const contextData = JSON.parse(fs.readFileSync('./context.json', 'utf8'));
            context = contextData.context || '';
        }
        
        res.json({ 
            status: 'success',
            context: context
        });
        
    } catch (error) {
        console.error('Erreur lors de la lecture du contexte:', error);
        res.json({ 
            status: 'success',
            context: ''
        });
    }
});

// Route principale du chat
app.post('/api/chat', async (req, res) => {
    try {
        const { message, history = [] } = req.body;

        if (!message || message.trim() === '') {
            return res.status(400).json({ error: 'Message requis' });
        }

        // R√©cup√©rer le contexte syst√®me
        let systemPrompt = null;
        if (fs.existsSync('./context.json')) {
            try {
                const contextData = JSON.parse(fs.readFileSync('./context.json', 'utf8'));
                systemPrompt = contextData.context || null;
            } catch (error) {
                console.log('Pas de contexte syst√®me d√©fini');
            }
        }

        // D√©terminer le type de template
        const templateType = config.templateType || 'chatml';
        
        // Construire le prompt selon le template
        const prompt = formatPrompt(message, history, templateType, systemPrompt);

        console.log('Template utilis√©:', templateType);
        console.log('Contexte syst√®me:', systemPrompt ? 'D√©fini' : 'Non d√©fini');
        console.log('Prompt envoy√©:', prompt);

        // Appeler llama.cpp
        const response = await callLlamaCpp(prompt, templateType);
        
        res.json({ 
            response: response.trim(),
            status: 'success',
            template: templateType,
            hasContext: !!systemPrompt
        });

    } catch (error) {
        console.error('Erreur lors du traitement:', error);
        res.status(500).json({ 
            error: 'Erreur interne du serveur',
            details: error.message 
        });
    }
});

// Fonction pour appeler llama.cpp
function callLlamaCpp(prompt, templateType) {
    return new Promise((resolve, reject) => {
        const args = [
            '-m', config.modelPath,
            '-p', prompt,
            '-n', config.maxTokens.toString(),
            '--temp', config.temperature.toString(),
            '-c', config.llamaArgs.ctx_size.toString(),
            '-t', config.llamaArgs.threads.toString(),
            '-b', config.llamaArgs.batch_size.toString(),
            '--no-display-prompt',
            '-e',  // Traiter les √©chappements
            '-s', '-1'  // Seed al√©atoire
        ];

        console.log('Commande llama.cpp:', config.llamaCppPath, args.join(' '));

        const llamaProcess = spawn(config.llamaCppPath, args);
        
        let output = '';
        let errorOutput = '';

        llamaProcess.stdout.on('data', (data) => {
            output += data.toString();
        });

        llamaProcess.stderr.on('data', (data) => {
            errorOutput += data.toString();
        });

        llamaProcess.on('close', (code) => {
            if (code === 0) {
                // Nettoyer la sortie selon le template
                let cleanedOutput = cleanOutput(output, templateType);
                
                // Si la r√©ponse est vide, fournir un message par d√©faut
                if (!cleanedOutput) {
                    cleanedOutput = 'D√©sol√©, je n\'ai pas pu g√©n√©rer une r√©ponse.';
                }
                
                resolve(cleanedOutput);
            } else {
                console.error('Erreur llama.cpp:', errorOutput);
                reject(new Error(`llama.cpp a √©chou√© avec le code ${code}: ${errorOutput}`));
            }
        });

        llamaProcess.on('error', (error) => {
            console.error('Erreur de spawn:', error);
            reject(new Error(`Impossible de lancer llama.cpp: ${error.message}`));
        });

        // Timeout de s√©curit√© 
        setTimeout(() => {
            llamaProcess.kill();
            reject(new Error('Timeout: llama.cpp a pris trop de temps (90s)'));
        }, 90000);
    });
}

// Route de sant√©
app.get('/api/health', (req, res) => {
    res.json({ 
        status: 'ok', 
        timestamp: new Date().toISOString(),
        model: config.modelPath,
        llamaPath: config.llamaCppPath,
        template: config.templateType || 'chatml'
    });
});

// Middleware de gestion d'erreurs
app.use((error, req, res, next) => {
    console.error('Erreur non g√©r√©e:', error);
    res.status(500).json({
        error: 'Erreur interne du serveur',
        message: error.message
    });
});

// D√©marrer le serveur
const PORT = config.port || 3000;
app.listen(PORT, () => {
    const templateType = config.templateType || 'chatml';
    const modelName = path.basename(config.modelPath, '.gguf');
    
    console.log(`üöÄ Serveur d√©marr√© sur le port ${PORT}`);
    console.log(`üì± Interface: http://localhost:${PORT}`);
    console.log(`üîó Version embeddable: http://localhost:${PORT}/embed`);
    console.log(`ü§ñ Mod√®le: ${modelName}`);
    console.log(`‚öôÔ∏è  llama.cpp: ${config.llamaCppPath}`);
    console.log(`üìù Template: ${templateType}`);
    console.log(`üéØ Contexte syst√®me: Disponible via l'interface`);
    console.log(`‚è∞ Timeout: 90 secondes`);
    console.log(`üßπ Nettoyage automatique des tokens de fin`);
    console.log(`üí¨ Premi√®re g√©n√©ration peut prendre 30-60 secondes`);
});
