const express = require('express');
const cors = require('cors');
const bodyParser = require('body-parser');
const { spawn } = require('child_process');
const path = require('path');
const fs = require('fs');

// Charger la configuration
const config = JSON.parse(fs.readFileSync('./config.json', 'utf8'));

const app = express();

// Middleware
app.use(cors());
app.use(bodyParser.json());
app.use(express.static('public'));

// Route principale
app.get('/', (req, res) => {
    res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

// Route pour la version embeddable
app.get('/embed', (req, res) => {
    res.sendFile(path.join(__dirname, 'public', 'embed.html'));
});

// Route pour rÃ©cupÃ©rer la configuration cÃ´tÃ© client
app.get('/api/config', (req, res) => {
    res.json({
        chatbot: config.chatbot
    });
});

// Route principale du chat
app.post('/api/chat', async (req, res) => {
    try {
        const { message, history = [] } = req.body;

        if (!message || message.trim() === '') {
            return res.status(400).json({ error: 'Message requis' });
        }

        // Construire le prompt avec le format ChatML de CroissantLLM
        let prompt = '';
        
        // Ajouter l'historique des messages prÃ©cÃ©dents
        if (history.length > 0) {
            history.forEach(msg => {
                if (msg.type === 'user') {
                    prompt += `<|im_start|>user\n${msg.content}<|im_end|>\n`;
                } else if (msg.type === 'bot') {
                    prompt += `<|im_start|>assistant\n${msg.content}<|im_end|>\n`;
                }
            });
        }
        
        // Ajouter le message actuel et commencer la rÃ©ponse de l'assistant
        prompt += `<|im_start|>user\n${message}<|im_end|>\n<|im_start|>assistant\n`;

        console.log('Prompt envoyÃ©:', prompt);

        // Appeler llama.cpp
        const response = await callLlamaCpp(prompt);
        
        res.json({ 
            response: response.trim(),
            status: 'success'
        });

    } catch (error) {
        console.error('Erreur lors du traitement:', error);
        res.status(500).json({ 
            error: 'Erreur interne du serveur',
            details: error.message 
        });
    }
});

// Fonction pour appeler llama.cpp
function callLlamaCpp(prompt) {
    return new Promise((resolve, reject) => {
        const args = [
            '-m', config.modelPath,
            '-p', prompt,
            '-n', config.maxTokens.toString(),
            '--temp', config.temperature.toString(),
            '-c', config.llamaArgs.ctx_size.toString(),
            '-t', config.llamaArgs.threads.toString(),
            '-b', config.llamaArgs.batch_size.toString(),
            '--no-display-prompt',
            '-e',  // Traiter les Ã©chappements
            '-s', '-1',  // Seed alÃ©atoire
            '--no-cnv'  // DÃ©sactiver le mode conversation automatique
        ];

        console.log('Commande llama.cpp:', config.llamaCppPath, args.join(' '));

        const llamaProcess = spawn(config.llamaCppPath, args);
        
        let output = '';
        let errorOutput = '';

        llamaProcess.stdout.on('data', (data) => {
            output += data.toString();
        });

        llamaProcess.stderr.on('data', (data) => {
            errorOutput += data.toString();
        });

        llamaProcess.on('close', (code) => {
            if (code === 0) {
                // Nettoyer la sortie : supprimer <|im_end|> et autres tokens
                let cleanOutput = output
                    .replace(/<\|im_end\|>/g, '')  // Supprimer les tokens de fin
                    .replace(/^\s*\n/, '')         // Enlever les nouvelles lignes au dÃ©but
                    .replace(/\n\s*$/, '')         // Enlever les nouvelles lignes Ã  la fin
                    .trim();
                
                // Si la rÃ©ponse est vide, fournir un message par dÃ©faut
                if (!cleanOutput) {
                    cleanOutput = 'DÃ©solÃ©, je n\'ai pas pu gÃ©nÃ©rer une rÃ©ponse.';
                }
                
                resolve(cleanOutput);
            } else {
                console.error('Erreur llama.cpp:', errorOutput);
                reject(new Error(`llama.cpp a Ã©chouÃ© avec le code ${code}: ${errorOutput}`));
            }
        });

        llamaProcess.on('error', (error) => {
            console.error('Erreur de spawn:', error);
            reject(new Error(`Impossible de lancer llama.cpp: ${error.message}`));
        });

        // Timeout de sÃ©curitÃ© 
        setTimeout(() => {
            llamaProcess.kill();
            reject(new Error('Timeout: llama.cpp a pris trop de temps (90s)'));
        }, 90000); // 90 secondes pour la premiÃ¨re gÃ©nÃ©ration
    });
}

// Route de santÃ©
app.get('/api/health', (req, res) => {
    res.json({ 
        status: 'ok', 
        timestamp: new Date().toISOString(),
        model: config.modelPath,
        llamaPath: config.llamaCppPath
    });
});

// Middleware de gestion d'erreurs
app.use((error, req, res, next) => {
    console.error('Erreur non gÃ©rÃ©e:', error);
    res.status(500).json({
        error: 'Erreur interne du serveur',
        message: error.message
    });
});

// DÃ©marrer le serveur
const PORT = config.port || 3000;
app.listen(PORT, () => {
    console.log(`ğŸš€ Serveur dÃ©marrÃ© sur le port ${PORT}`);
    console.log(`ğŸ“± Interface: http://localhost:${PORT}`);
    console.log(`ğŸ”— Version embeddable: http://localhost:${PORT}/embed`);
    console.log(`ğŸ¤– ModÃ¨le: ${config.modelPath}`);
    console.log(`âš™ï¸  llama.cpp: ${config.llamaCppPath}`);
    console.log(`â° Timeout: 90 secondes`);
    console.log(`ğŸ’¡ Format utilisÃ©: ChatML (<|im_start|>/<|im_end|>)`);
    console.log(`ğŸ’¬ PremiÃ¨re gÃ©nÃ©ration peut prendre 30-60 secondes`);
});
